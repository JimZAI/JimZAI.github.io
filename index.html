<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ji Zhang</title> 
 
    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
     
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Ji Zhang 
                </p>
                <p> I'm Ji Zhang (张继), an Assistant Professor in School of Computing and Artificial Intelligence, Southwest Jiaotong University (SWJTU). I received my Ph.D degree in School of Computer Science and Engineering, University of Electronic Science and Technology of China (UESTC) in 2024, advised by Prof.  <a href="https://scholar.google.com/citations?user=F5Zy9V4AAAAJ&hl=en&oi=ao">Jingkuan Song</a> and Prof. <a href="https://scholar.google.com/citations?user=zsm2dpYAAAAJ&hl=en&oi=ao">Lianli Gao</a>.
                </p>
                <p>
                My research interests include Robotics, Embodied AI and Few-shot Learning. I published several papers on top conferences or journals, e.g., CVPR, ICCV, ICML, ACM MM, IEEE TIP.
		I'm especially interested in developing advanced algorithms that enable efficient and effective adaptation of vision-language models and robotic foundation models to diverse downstream tasks. If you are also interested in related topics, please do not hesitate to reach out.
		</p>
                <p style="text-align:left">
                  <a href="https://scholar.google.com/citations?hl=en&user=F9dKrEwAAAAJ&view_op=list_works&sortby=pubdate">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/JimZAI">Github</a> &nbsp;/&nbsp;
		  <a href="mailto:jizhang.jim@gmail.com">Email</a> &nbsp;/&nbsp;
		  <a href="images/zhangjiwechat.pdf">WeChat</a> 
                </p> 
              </td>
	      <td style="padding:2.5%;width:40%;max-width:40%"> 
                <a href="images/zhangji.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/zhangji.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:left">
                           <h2>News</h2> 
		           <p>
			   [04/2025] I have been invited to be a PC member for NeurIPS'25. <br/>
		           [03/2025] I have been invited to be a PC member for ICCV'25. <br/>
		           [03/2025] I have been invited to be a PC member for ACM MM'25. <br/>
			   [02/2025] One paper about Prompt Tuning is accepted by CVPR'25. <br/>
		           [11/2024] I have been invited to be a PC member for CVPR'25. <br/>
			   [08/2024] I have been invited to be a PC member for ICLR'25. <br/>
		           [02/2024] One paper about Prompt Tuning is accepted by CVPR'24. <br/>
                           [10/2023] One paper about OOD Detection is accepted by IEEE TIP. <br/>
                           [10/2023] I'm awarded the "Graduate Chinese National Scholarship". <br/>
			   [07/2023] One paper about FSL is accepted by ICCV'23. <br/>
			   [04/2023] One paper about FSL is accepted by ICML'23. <br/>
			   [06/2022] Two papers about FSL and Continue Learning are accepted by ACM MM'22. <br/>
			   [03/2022] One paper about FSL is accepted by IEEE TCSVT. <br/>
			   [06/2021] One paper about FSL is accepted by ACM MM'21. 
			   </p>
		           
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		 
<h2>&nbsp;&nbsp;&nbsp; Selected Papers</h2> 

<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhangjicvpr25.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2309.07439">
      <span class="papertitle">Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves
</span>
    </a>
    <br> Shihan Wu, <strong>Ji Zhang^#</strong>, Pengpeng Zeng, Lianli Gao, Jingkuan Song, Heng Tao Shen
    <br> IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025
    <br> [<a href="https://arxiv.org/abs/2412.11509">Paper</a>][<a href="https://github.com/Koorye/SkipTuning">Code</a>]
    <p>
    Achieving effective and efficient adaptation of large pre-trained vision-language models.
    </p>
  </td>
</tr>  
		  
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhagnjicvpr.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2309.07439">
      <span class="papertitle">DePT: Decoupled Prompt Tuning
</span>
    </a>
    <br> <strong>Ji Zhang</strong>*, Shihan Wu*, Lianli Gao, Heng Tao Shen, Jingkuan Song
    <br> IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024
    <br> [<a href="https://arxiv.org/abs/2309.07439">Paper</a>][<a href="https://github.com/Koorye/DePT">Code</a>]
    <p>
    Overcoming the base-new tradeofff (BNT) problem for existing prompt tuning methods.
    </p>
  </td>
</tr>          

<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhangjitip.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2308.10239">
      <span class="papertitle">From Global to Local: Multi-scale Out-of-distribution Detection
</span>
    </a>
    <br> <strong>Ji Zhang</strong>, Lianli Gao, Bingguang Hao, Hao Huang, Jingkuan Song, Heng Tao Shen
    <br> IEEE Transactions on Image Procesing (<strong>TIP</strong>), 2023 
    <br> [<a href="https://arxiv.org/pdf/2308.10239">Paper</a>][<a href="https://github.com/JimZAI/MODE-OOD">Code</a>]
    <p>
    Leveraging both global visual information and local region details of images to maximally benefit OOD detection.
    </p>
  </td>
</tr>
  
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhangjiiccv.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2303.06315">
      <span class="papertitle">DETA: Denoised Task Adaptation for Few-shot Learning
</span>
    </a>
    <br> <strong>Ji Zhang</strong>, Lianli Gao, Xu Luo, Heng Tao Shen, Jingkuan Song
    <br> IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2023 
	  <br> [<a href="https://arxiv.org/pdf/2303.06315">Paper</a>][<a href="https://github.com/JimZAI/DETA">Code</a>]
    <p>
     Tacking both the X-noise (i.e., image noise) and the Y-noise (i.e., label noise) in a unified framework for test-time few-shot tasks.
    </p>
  </td>
</tr>  

<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhangjiicml.jpeg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2301.12246">
      <span class="papertitle">A Closer Look at Few-shot Classification Again
</span>
    </a>
    <br> Xu Luo*, Hao Wu*, <strong>Ji Zhang</strong>, Lianli Gao, Jing Xu, Jingkuan Song
    <br> International Conference on Machine Learning (<strong>ICML</strong>), 2023 
	  <br> [<a href="https://arxiv.org/pdf/2301.12246">Paper</a>][<a href="https://github.com/Frankluox/CloserLookAgainFewShot">Code</a>]
    <p>
     Empirically proving the disentanglement of training and test-time adaptation algorithms in few-shot classification.
    </p>
  </td>
</tr>  

<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhangjisetrcl.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547835">
      <span class="papertitle">Free-lunch for Cross-domain Few-shot learning: Style-aware Episodic Training with Robust Contrastive Learning
</span>
    </a>
    <br> <strong>Ji Zhang</strong>, Jingkuan Song, Lianli Gao, Heng Tao Shen
    <br> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022 
	  <br> [<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547835">Paper</a>][<a href="https://github.com/JimZAI/SET-RCL">Code</a>]
    <p>
     Addressing the side-effect of style-shift between tasks from source and target domains.
    </p>
  </td>
</tr>  
		  
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhangjicc2.png' width="160">
    </div> 
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548054">
      <span class="papertitle">Class Gradient Projection for Continual Learning
</span>
    </a>
    <br> Cheng Chen,  <strong>Ji Zhang</strong>, Jingkuan Song, Lianli Gao
    <br> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022 
	  <br> [<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548054">Paper</a>][<a href="https://github.com/zackschen/CGP">Code</a>]
    <p>
     Projecting the gradient update orthogonal to the gradient subspace of individual classes to mitigate the catastrophic forgetting issue in continual learning.
    </p>
  </td>
</tr>   

<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhangjisepmeta.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ieeexplore.ieee.org/abstract/document/9745972/">
      <span class="papertitle">Progressive Meta-learning with Curriculum
</span>
    </a>
    <br> <strong>Ji Zhang</strong>, Jingkuan Song, Lianli Gao, Ye Liu, Heng Tao Shen
    <br> IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), 2022 
	  <br> [<a href="https://ieeexplore.ieee.org/abstract/document/9745972/">Paper</a>][<a href="https://github.com/JimZAI/SepMeta">Code</a>]
    <p>
     An extended version the ACM MM'21 paper, where the curriculum is effectively integrated as a regularization term into the objective so that the meta-learner can measure the hardness of tasks adaptively.
    </p>
  </td>
</tr>  

<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='zhangji'>
      <img src='images/zhangjicubmeta.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475335">
      <span class="papertitle">Curriculum-based Meta-learning
</span>
    </a>
    <br> <strong>Ji Zhang</strong>, Jingkuan Song, Yazhou Yao, Lianli Gao
    <br> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021
	  <br> [<a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475335">Paper</a>][<a href="https://github.com/JimZAI/CubMeta">Code</a>]
    <p>
     Progressively improving the meta-learner by performing episodic training on simulating tasks from easy to hard, i.e., in a curriculum learning manner.
    </p>
  </td>
</tr>  
		  
</tbody></table>

<h2>&nbsp;&nbsp;&nbsp; Academic Service</h2> 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
		I'm a reviewer of several top journals/conferences, e.g. CVPR, ECCV, ACM MM, AAAI, IEEE TIP, IEEE TMM, IEEE TCSVT.
              </td>
            </tr>
          </tbody></table>

<table  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                 This well-designed template is borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jonbarron</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
		
        </td>
      </tr>
    </table>
  </body>
</html>
